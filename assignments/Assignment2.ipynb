{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/SocialComplexityLab/socialgraphs2021/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc.), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Tuesday November the 2nd, 2021 at 23:59. Hand in your IPython notebook file (with extension `.ipynb`) via http://peergrade.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This year's Assignment 2 is all about analyzing the network of The Legend of Zelda: Breath of the Wild.\n",
    "\n",
    "Note that this time I'm doing the exercises slightly differently in order to clean things up a bit. The issue is that the weekly lectures/exercises include quite a few instructions and intermediate results that are not quite something you guys can meaningfully answer. \n",
    "\n",
    "Therefore, in the assignment below, I have tried to reformulate the questions from the weekly exercises into something that is (hopefully) easier to answer. *Then I also note which lectures each question comes from*; that way, you can easily go back and find additional tips & tricks on how to solve things ðŸ˜‡\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Building the network \n",
    "\n",
    "To create our network, we downloaded the Zelda Wiki pages for all characters in BotW (during Week 4) and linked them via the hyperlinks connecting pages to each other. To achieve this goal we have used regular expressions!\n",
    "\n",
    "> * Explain the strategy you have used to extract the hyperlinks from the Wiki-pages, assuming that you have already collected the pages with the Zelda API.\n",
    "> * Show the regular expression(s) you have built and explain in details how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Network visualization and basic stats\n",
    "\n",
    "Visualize the network (from lecture 5) and calculate stats (from lecture 4 and 5). For this exercise, we assume that you've already generated the BotW network and extracted the giant connected component. Use the GCC to report the results.\n",
    "\n",
    "_Exercise 1a_: Stats (see lecture 4 and 5 for more hints)\n",
    "\n",
    "> * What is the number of nodes in the network? \n",
    "> * What is the number of links?\n",
    "> * Who is the top connected character in BotW? (Report results for the in-degrees and out-degrees). Comment on your findings. Is this what you would have expected?\n",
    "> * Who are the top 5 most connected allies (again in terms of in/out-degree)? \n",
    "> * Who are the top 5 most connected enemies -- bosses included -- (again in terms of in/out-degree)?\n",
    "> * Plot the in- and out-degree distributions. \n",
    ">   * What do you observe? \n",
    ">   * Can you explain why the in-degree distribution is different from the out-degree distribution?\n",
    "> * Find the exponent of the degree distribution (by using the `powerlaw` package) for the in- and out-degree distribution. What does it say about our network?\n",
    "> * Compare the degree distribution of the undirected graph to a *random network* with the same number of nodes and probability of connection *p*. Comment your results.\n",
    "\n",
    "_Exercise 1b_: Visualization (see lecture 5 for more hints)\n",
    "\n",
    "> * Create a nice visualization of the total (undirected) network:\n",
    ">   * Color nodes according to the role;\n",
    ">   * Scale node-size according to degree;\n",
    ">   * Get node positions based on the Force Atlas 2 algorithm;\n",
    ">   * Whatever else you feel like that would make the visualization nicer.\n",
    "> * Describe the structure you observe. Can you identify nodes with a privileged position in the network? Do you observe chains of connected nodes? Do you see any interesting group of nodes (can you guess who's involved)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word-clouds\n",
    "\n",
    "Create your own version of the word-clouds (from lecture 7). For this exercise we assume you know how to download and clean text from the ZeldaWiki pages.\n",
    "\n",
    "Here's what you need to do:\n",
    "> * Create a word-cloud for each race of the [five champions of Hyrule](https://zelda.fandom.com/wiki/Champions) (i.e. Hylian, Zora, Goron, Gerudo, and Rito) according to either TC-IDF. Feel free to make it as fancy as you like. Explain your process and comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/olinezachariassen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/olinezachariassen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/olinezachariassen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fect text with the zelda APi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseurl = \"https://zelda.fandom.com/api.php?action=query&titles={}&prop=revisions&rvprop=content&rvslots=*&format=json\"\n",
    "titles = [\"Hylian\", \"Zora\", \"Goron\", \"Gerudo\",\"Rito\" ]\n",
    "pagedict = {}\n",
    "stringdict = {}\n",
    "for title in titles:\n",
    "    response = requests.get(baseurl.format(title))\n",
    "    pagedict[title] = response.json()\n",
    "    key = next(iter(response.json()[\"query\"][\"pages\"].keys()))\n",
    "    stringdict[title] = response.json()[\"query\"][\"pages\"][key][\"revisions\"][0]['slots']['main']['*']\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stringdict2 = {}\n",
    "\n",
    "for title in titles:\n",
    "    text = stringdict[title]\n",
    "    \n",
    "    #text = re.sub(r'\\S*@\\S*\\s*', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\{\\{.*?\\}\\}', '', text)\n",
    "    text = re.sub(r'\\[\\[.*?\\]\\]', '', text)\n",
    "    text = re.sub(r'\\<.*?\\>', '', text)\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    #tokens = [word for word in text if not word in stopwords.words('english')]\n",
    " \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stringdict2[title]=  tokens\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here comes TDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the lists to strings to be able to use the wourdCloud library  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'false' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-222cd9de5ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#string = \" \".join(stringdict2[title])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'false' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "def term_count(titleList):\n",
    "    termCount = {}\n",
    "    for word in titleList:\n",
    "        if word in termCount:\n",
    "            termCount[word] +=1\n",
    "        else: \n",
    "            termCount[word]= 1 \n",
    "\n",
    "    return termCount\n",
    "\n",
    "collectionText = nltk.TextCollection([stringdict2['Hylian'], stringdict2['Zora'], stringdict2['Goron'],stringdict2['Gerudo'],stringdict2['Rito']])\n",
    "\n",
    "\n",
    "\n",
    "def idf(text):\n",
    "    idf= {}\n",
    "    for word in text:\n",
    "        idf[word] = text.idf(word)\n",
    "    return idf\n",
    "        \n",
    "\n",
    "def tc_idf(title):\n",
    "    idfs = idf(collectionText)\n",
    "    tc_idf= {}\n",
    "    for word in stringdict2[title]:\n",
    "        tc_idf[word] = int(math.ceil(idfs[word])) * term_count(stringdict2[title])[word]\n",
    "    return tc_idf\n",
    "\n",
    "hylian_tc_idf= tc_idf('Hylian')\n",
    "zora_tc_idf= tc_idf('Zora')\n",
    "goron_tc_idf= tc_idf('Goron')\n",
    "gerudo_tc_idf= tc_idf('Gerudo')\n",
    "rito_tc_idf= tc_idf('Rito')\n",
    "tc_idfs = [hylian_tc_idf, zora_tc_idf, goron_tc_idf, gerudo_tc_idf,rito_tc_idf]\n",
    "        \n",
    "for tcidf in tc_idfs:  \n",
    "    strng = \"\"\n",
    "    for key,value in tcidf.items():\n",
    "        strng+= ((key+ \" \") * value)\n",
    "    \n",
    "    \n",
    "    #string = \" \".join(stringdict2[title])\n",
    "    wordcloud = WordCloud(background_color=\"white\",collocations=False).generate(strng)\n",
    "    img=plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    print( title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Communities and TF-IDF\n",
    "\n",
    "Find communities and compute their associated TF-IDF (from lecture 7 and 8).\n",
    "\n",
    "Here's what you need to do:\n",
    "> * Explain the Louvain algorithm and how it finds communities in a newtork.\n",
    "> * Explain how you chose to identify the communities: Which algorithm did you use? (if you did not use the Louvain method, explain how the method you have used works) \n",
    "> * Comment your results:\n",
    ">   * How many communities did you find in total?\n",
    ">   * Compute the value of modularity with the partition created by the algorithm.\n",
    ">   * Plot the distribution of community sizes.\n",
    "> * For the 5 largest communities, create TF-IDF based rankings of words in each community. \n",
    ">   * There are many ways to calculate TF-IDF, explain how you've done it and motivate your choices.\n",
    ">   * List the 5 top words for each community according to TF.\n",
    ">   * List the 5 top words for each community accourding to TF-IDF. Are these words more descriptive of the community than just the TF? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Sentiment of communities\n",
    "\n",
    "Analyze the sentiment of the communities (lecture 8). Here, we assume that you've successfully identified communities.  More tips & tricks can be found, if you take a look at Lecture 8's exercises.\n",
    "\n",
    "A couple of additional instructions you will need below:\n",
    "* We name each community by its three most connected characters.\n",
    "* Average the average sentiment of the nodes in each community to find a community level sentiment.\n",
    "\n",
    "Here's what you need to do (repeat these steps and report your results for **both LabMT and VADER**):\n",
    "> * Calculate and store sentiment for every character\n",
    "> * Create a histogram of all character's associated sentiments.\n",
    "> * What are the 10 characters with happiest and saddest pages?\n",
    "\n",
    "Now, compute the sentiment of communities: \n",
    "> * What are the three happiest communities according to the LabMT wordlist approach? What about VADER?\n",
    "> * What are the three saddest communities according to the LabMT wordlist approach? What about VADER?\n",
    "> * Create a bar plot showing the average sentiment of each community and add error-bars using the standard deviation for both methods. \n",
    "> * Explain the difference between the two methods and compare the results you have obtained above.\n",
    "> * What is the advantage of using a rule-based method over the dictionary-based approach? "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
